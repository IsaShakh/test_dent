from load_categories import load_categories_from_file
from category_parser import get_products_from_category
from competitor_search import get_product_title, search_on_all_competitors
from process_analog_url import process_competitor_url
from to_google import upload_to_google_sheets
from meta_info import extract_info
import re

not_found = []


def generate_search_query(title: str) -> str:
    cleaned = re.sub(r"[^\w\s-]", "", title)
    cleaned = re.sub(r"\s+", " ", cleaned).strip()

    words = cleaned.split()

    stop_words = {"refill", "kit", "xl", "ho", "ml", "g", "—à—Ç", "–≥", "n", "try", "in"}
    keywords = [w for w in words if len(w) > 2 and w.lower() not in stop_words]

    if not keywords:
        keywords = words[:2]

    return " ".join(keywords[:3])

competitor_sites = [
    "el-dent.ru",
    "stomatorg.ru",
    "nika-dent.ru",
    "aveldent.ru",
    "stomdevice.ru"
]

category_urls = load_categories_from_file("categories.txt")
all_results = []

for category_url in category_urls:
    print(f"\n–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_url}")
    product_urls = get_products_from_category(category_url)
    print(f"–ù–∞–π–¥–µ–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(product_urls)}")

    for my_url in product_urls:
        print(f"\nüîπ –¢–æ–≤–∞—Ä: {my_url}")
        title = get_product_title(my_url)
        print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {title}")
        if not title:
            print("–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º.")
            continue

        search_query = generate_search_query(title)
        print(f"–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {search_query}")

        competitor_url = search_on_all_competitors(search_query, competitor_sites)
        if not competitor_url:
            print("   –ê–Ω–∞–ª–æ–≥ –Ω–µ –Ω–∞–π–¥–µ–Ω")
            not_found.append(my_url)

            continue

        result = process_competitor_url(competitor_url, my_url)
        if result:
            print("–û–ø–∏—Å–∞–Ω–∏–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ")
            try:
                upload_to_google_sheets(
                    data_list=[result], 
                    creds_path="D:\google_cred.json",
                    spreadsheet_name="desc_autogen"
                )
                print("   üì• –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ Google –¢–∞–±–ª–∏—Ü—É")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –≤ Google –¢–∞–±–ª–∏—Ü—É: {e}")
            import time
            time.sleep(1.2)
        else:
            print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è")
        
        
if not_found:
    with open("not_found.txt", "w", encoding="utf-8") as f:
        f.write("–°—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω –∞–Ω–∞–ª–æ–≥:\n\n")
        for url in not_found:
            f.write(url + "\n")
    print(f"\n–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(not_found)} –Ω–µ–Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ not_found.txt")
else:
    print("\n–í—Å–µ —Ç–æ–≤–∞—Ä—ã —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∏ –Ω–∞–π–¥–µ–Ω—ã")
